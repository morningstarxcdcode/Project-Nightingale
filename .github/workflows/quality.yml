name: Code Quality and Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'

env:
  PYTHON_VERSION: '3.9'
  POETRY_VERSION: '1.4.2'

jobs:
  lint-and-format:
    name: Linting and Code Formatting
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install black isort flake8 mypy bandit safety
        pip install -r requirements.txt
        if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi
    
    - name: Check code formatting with Black
      run: |
        black --check --diff src/ scripts/ gui/ tests/
    
    - name: Check import sorting with isort
      run: |
        isort --check-only --diff src/ scripts/ gui/ tests/
    
    - name: Lint with flake8
      run: |
        flake8 src/ scripts/ gui/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src/ scripts/ gui/ tests/ --count --max-complexity=10 --max-line-length=88 --statistics
    
    - name: Type checking with mypy
      run: |
        mypy src/ scripts/ gui/ --ignore-missing-imports
    
    - name: Security check with bandit
      run: |
        bandit -r src/ scripts/ gui/ -f json -o bandit-report.json
    
    - name: Check dependencies for known security vulnerabilities
      run: |
        safety check --json --output safety-report.json
      continue-on-error: true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  test:
    name: Run Tests
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.9', '3.10', '3.11', '3.12']
        exclude:
          # Reduce matrix size for faster CI
          - os: windows-latest
            python-version: '3.9'
          - os: macos-latest
            python-version: '3.9'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-${{ matrix.python-version }}-pip-
    
    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        sudo apt-get update
        sudo apt-get install -y python3-tk
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov pytest-mock pytest-xdist
        pip install -r requirements.txt
        if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi
      shell: bash
    
    - name: Run tests with pytest
      run: |
        pytest tests/ -v --cov=src --cov=scripts --cov=gui --cov-report=xml --cov-report=html --cov-report=term-missing
    
    - name: Upload coverage reports to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.9'
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
    
    - name: Upload coverage HTML report
      uses: actions/upload-artifact@v3
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.9'
      with:
        name: coverage-html-report
        path: htmlcov/

  integration-test:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: test
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        sudo apt-get update && sudo apt-get install -y python3-tk
    
    - name: Test main application
      run: |
        python src/main.py
    
    - name: Test imports and module structure
      run: |
        python -c "
        import sys, os
        sys.path.insert(0, '.')
        
        # Test all imports work
        from src.main import main
        from scripts.ai_model import simple_ai_model
        from scripts.ai_utilities import preprocess_data, evaluate_model
        from scripts.config import get_config
        from scripts.exceptions import NightingaleError
        
        print('‚úÖ All imports successful')
        
        # Test basic functionality
        result = simple_ai_model('test input')
        assert 'Processed data: test input' in result
        print('‚úÖ AI model working')
        
        config = get_config()
        assert config.get('app.name') == 'Project Nightingale'
        print('‚úÖ Configuration system working')
        
        print('üéâ Integration tests passed!')
        "
    
    - name: Test GUI imports (without display)
      run: |
        python -c "
        try:
            # This will fail on headless systems but we can test imports
            import sys, os
            sys.path.insert(0, '.')
            
            # Mock tkinter to avoid display issues
            import unittest.mock
            with unittest.mock.patch('tkinter.Tk'):
                from gui.main_gui import Application
                print('‚úÖ GUI imports successful')
        except ImportError as e:
            print(f'‚ö†Ô∏è  GUI import warning (expected on headless): {e}')
        "

  performance-test:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: test
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install memory_profiler psutil
    
    - name: Run performance benchmarks
      run: |
        python -c "
        import time
        import sys
        import os
        sys.path.insert(0, '.')
        
        from scripts.ai_model import simple_ai_model
        from scripts.ai_utilities import preprocess_data, evaluate_model
        
        # Test AI model performance
        start_time = time.time()
        for i in range(1000):
            result = simple_ai_model(f'test input {i}')
        ai_time = time.time() - start_time
        
        # Test preprocessing performance
        start_time = time.time()
        for i in range(10000):
            result = preprocess_data(f'  TEST DATA {i}  ')
        preprocess_time = time.time() - start_time
        
        # Test evaluation performance
        predictions = list(range(10000))
        actuals = list(range(10000))
        start_time = time.time()
        accuracy = evaluate_model(predictions, actuals)
        eval_time = time.time() - start_time
        
        print(f'üìä Performance Results:')
        print(f'  AI Model (1000 calls): {ai_time:.3f}s ({ai_time/1000*1000:.3f}ms per call)')
        print(f'  Preprocessing (10000 calls): {preprocess_time:.3f}s ({preprocess_time/10000*1000:.3f}ms per call)')
        print(f'  Evaluation (10000 items): {eval_time:.3f}s')
        
        # Performance assertions
        assert ai_time < 5.0, f'AI model too slow: {ai_time:.3f}s'
        assert preprocess_time < 2.0, f'Preprocessing too slow: {preprocess_time:.3f}s'
        assert eval_time < 1.0, f'Evaluation too slow: {eval_time:.3f}s'
        
        print('‚úÖ All performance tests passed!')
        "

  documentation:
    name: Documentation Check
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pydocstyle doc8
        pip install -r requirements.txt
    
    - name: Check docstring style
      run: |
        pydocstyle src/ scripts/ gui/ --convention=google
      continue-on-error: true
    
    - name: Validate README and docs
      run: |
        # Check that README exists and has basic sections
        if [ ! -f README.md ]; then
          echo "‚ùå README.md not found"
          exit 1
        fi
        
        # Check for basic sections in README
        if ! grep -q "# Project Nightingale" README.md; then
          echo "‚ùå README missing project title"
          exit 1
        fi
        
        echo "‚úÖ Documentation checks passed"

  build-package:
    name: Build Package
    runs-on: ubuntu-latest
    needs: [lint-and-format, test]
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build wheel setuptools
    
    - name: Build package
      run: |
        python -m build
    
    - name: Check package
      run: |
        pip install twine
        twine check dist/*
    
    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: dist-packages
        path: dist/

  notify-success:
    name: Notify Success
    runs-on: ubuntu-latest
    needs: [lint-and-format, test, integration-test, performance-test, documentation, build-package]
    if: success()
    steps:
    - name: Success notification
      run: |
        echo "üéâ All quality checks passed!"
        echo "‚úÖ Code formatting and linting"
        echo "‚úÖ Tests across multiple Python versions and OS"
        echo "‚úÖ Integration tests"
        echo "‚úÖ Performance benchmarks"
        echo "‚úÖ Documentation checks"
        echo "‚úÖ Package building"